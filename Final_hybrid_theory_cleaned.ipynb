{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b46_g0fsJ7rU"
   },
   "source": [
    "lstm model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcUmwtPfaPf2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d11b13db-dedd-4fb5-8827-d7e0347be025"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sample Conversation: ['Can we make this quick ? Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad . Again .', \"Well , I thought we'd start with pronunciation , if that's okay with you .\"]\n",
      "Total training pairs: 25152\n",
      "Sample training pair: ('Can we make this quick ? Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad . Again .', \"Well , I thought we'd start with pronunciation , if that's okay with you .\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Define paths to the data files\n",
    "lines_file_path = 'movie_lines.txt'\n",
    "conversations_file_path = 'movie_conversations.txt'\n",
    "\n",
    "dataset_size = 10000  # Adjust this value as needed\n",
    "\n",
    "# Function to load and process movie lines\n",
    "def load_movie_lines(file_path):\n",
    "    id_to_line = {}\n",
    "    with open(file_path, 'r', encoding='iso-8859-1') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(\" +++$+++ \")\n",
    "            if len(parts) == 5:\n",
    "                line_id, text = parts[0], parts[4]\n",
    "                # Clean text\n",
    "                text = re.sub(r\"([?.!,])\", r\" \\1 \", text)  # Space out punctuation\n",
    "                text = re.sub(r'\\s+', ' ', text).strip()\n",
    "                id_to_line[line_id] = text\n",
    "    return id_to_line\n",
    "\n",
    "# Function to load and process conversations with a limit on the number of samples\n",
    "def load_conversations(file_path, id_to_line, limit=None):\n",
    "    conversations = []\n",
    "    with open(file_path, 'r', encoding='iso-8859-1') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(\" +++$+++ \")\n",
    "            if len(parts) == 4:\n",
    "                # Extract conversation line IDs and get corresponding text\n",
    "                line_ids = eval(parts[3])  # Converts string list to list\n",
    "                conv = [id_to_line[line_id] for line_id in line_ids if line_id in id_to_line]\n",
    "                if len(conv) > 1:\n",
    "                    conversations.append(conv)\n",
    "                # Stop if limit is reached\n",
    "                if limit and len(conversations) >= limit:\n",
    "                    break\n",
    "    return conversations\n",
    "\n",
    "# Load movie lines and conversations with a dataset size limit\n",
    "id_to_line = load_movie_lines(lines_file_path)\n",
    "conversations = load_conversations(conversations_file_path, id_to_line, limit=dataset_size)\n",
    "\n",
    "# Example: Display first conversation pair\n",
    "print(\"Sample Conversation:\", conversations[0][:2])\n",
    "\n",
    "# Create training pairs: (input, response) for each conversation\n",
    "train_pairs = []\n",
    "for conv in conversations:\n",
    "    for i in range(len(conv) - 1):\n",
    "        input_text, response_text = conv[i], conv[i + 1]\n",
    "        train_pairs.append((input_text, response_text))\n",
    "\n",
    "print(\"Total training pairs:\", len(train_pairs))\n",
    "print(\"Sample training pair:\", train_pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YV1rXFVWNTYJ",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "16ea2281d358479e926226c0373dd4ef",
      "f1a1fdc751d24d8d879912dff00222ea",
      "350c832f4253465095f82bd3860b9c8c",
      "23202f7ed3114587bc2b3b20be9cb952",
      "359fb26e906a4e729dda7c9fde5c64ff",
      "22a4404124594d4baabf3c6371532053",
      "04b89a6747614cf9abf11d7a751692ce",
      "4180bb8b9dd146dea131cd0421d48f79",
      "eca5ba7e16a341958e39cfdecdf8ffad",
      "425bfcc10ab4420b889af5685315524a",
      "8af8706db6a24c53a262b3667867365e",
      "97e9146a6eb7434eb7d5b544aa86d36c",
      "cd072d2e0e1847c284bfa665b6df974f",
      "4205b9365ae84a849dac9000ed7dc4ba",
      "8fdcb4d29310403cb6a55f910fc2a063",
      "e5f279f9a06341639bb3269b16bc4b9c",
      "b9cc4f278242450fb5f039d5ce56d953",
      "b1ed3280c07c4ba19b681c741abe6712",
      "a07a96c67e214b95b0296f150ec6a680",
      "8ba0d3b3e9244761be849539ae54263b",
      "fc404ffb3edd402a889ec28614848cdb",
      "3db666f334654ad28d976ba92c1dcccf",
      "edd47b1e2dd44674b5d99c40deff65b5",
      "c57c54baeee649519b1ef2f77a9ff99c",
      "defdfbc23562426caef45dcad3a88b87",
      "a10690c56d194390b9cca92950bdf450",
      "276339cef38f4eacb04df8a7bab1fccb",
      "262a0202a2c3479a9e607d931bfcae93",
      "5e763f407a814230a8820a1f6cadcc30",
      "a0b9e1c239c043ebb56e9eb086504fcc",
      "eb79142fe2f14a01a292e3b7a773447f",
      "4cab5b519ea64357bb59bce3a0ce9b61",
      "6d538895c48c43aa96c4b58453792710",
      "53ac3eb29c474f10a15ee3bcad73e1fb",
      "a3bed5cbcef44d1dad223eb876c3136d",
      "c292a1c0eda7454abfc8b5c0f27c0003",
      "31ee783339f541a1aa81a45b20ae04d5",
      "dfc42ead7e8542f6ac63e31b848f3f32",
      "17c1b34ed9994571bafa36cd27e8a686",
      "835e94b750de44edaf6d4d243042ea3b",
      "88b3df6c6d8f4047a8d5eb78669c5a02",
      "2b12d95a862941c591ac0cccbcf4183e",
      "a6a691dc649d49e8a9980073d199d473",
      "e9b29fe59d4242d78464cd964d3b8bf1",
      "bb0d708561bf4770979be84d4858b609",
      "93aa0c32c2d74dbab2dc1c44238b5e1c",
      "73086ba203994ee4bd78e7bfa126f5f1",
      "16ff6cca54294e259cdd291f5f23bb81",
      "e2f64dc66de34cc0b04d2569df087482",
      "79d7b1fb2245422284b6c0e27701e002",
      "f6131a12526a458a9702ae09f1118489",
      "f452c181985342888e8854a22231ea8b",
      "32b0dd0129b946a98020c3822cab3cd9",
      "531d3114450f47759cdfca95e0b54b3b",
      "53a6f3f9693f4699b8923c18585c7a74",
      "00050350432d42239485c804514e145a",
      "5aa6592333df473ab50873d8f71ac618",
      "8a3c0e1b66a54fb6a45ebd05fd6fc0bc",
      "5154cf3792bb4b168e42fc76df5c56fc",
      "bb9119c85fe444479cb430b2f76a7bc5",
      "519b9a440f204d8d9681370eea8f9314",
      "00ae57b7bb33497fb685669b537666bc",
      "1cb5d4bb3738447ca828245a0814f38f",
      "2a2a2b74f14d43bb99bfddbf441c1e67",
      "9e78532626ec41fca009930d0dbc31d6",
      "8f7f0b96308c4ec38c324391d91162e9",
      "d475d6bab2be4387a0e6b8479993b0fb",
      "fd31b2e08e6240df8aaf836e71a4d086",
      "74f88f8ba8b04802a23e8acb6ae44d60",
      "7c56b7b510e1431aba6548be9303ab25",
      "1fb1bafbb8644eac97809ef1f8c78aa2",
      "95a0fd27efd44d50aba12fd0fd7ef336",
      "0794e9ed8382478f8a6002031a01a33e",
      "40ff5d73c3a24173ae68746303435f9d",
      "de7f727a4f444cef9c2294d856fd360f",
      "39dd252d647f48718003b1037cfd9341",
      "d879045ae3f64c2cb1f423fcbcf2b30c"
     ]
    },
    "outputId": "c8615529-7f6f-4a76-86bf-a01bc7101b97"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "16ea2281d358479e926226c0373dd4ef"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "97e9146a6eb7434eb7d5b544aa86d36c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "edd47b1e2dd44674b5d99c40deff65b5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "53ac3eb29c474f10a15ee3bcad73e1fb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bb0d708561bf4770979be84d4858b609"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00050350432d42239485c804514e145a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d475d6bab2be4387a0e6b8479993b0fb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20241107_044521-2r1cyakt</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vijaymehra8986-lovely-professional-university/huggingface/runs/2r1cyakt' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/vijaymehra8986-lovely-professional-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/vijaymehra8986-lovely-professional-university/huggingface' target=\"_blank\">https://wandb.ai/vijaymehra8986-lovely-professional-university/huggingface</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/vijaymehra8986-lovely-professional-university/huggingface/runs/2r1cyakt' target=\"_blank\">https://wandb.ai/vijaymehra8986-lovely-professional-university/huggingface/runs/2r1cyakt</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7500/7500 08:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.268200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.084200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.927400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.949000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.939000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.908600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.837000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.827700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.821100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.839200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.828400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.826700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>3.803400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.767800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.707100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>3.722500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>3.757100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>3.710200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>3.647600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.688200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>3.667700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>3.668500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>3.788900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>3.614000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.703700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>3.365100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>3.376800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>3.382200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>3.351700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.340700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>3.352900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>3.448600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>3.336200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>3.382400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.368000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>3.310700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>3.339100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>3.289000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>3.292400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.273800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>3.311800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>3.315900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>3.305700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>3.305500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.307800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>3.323600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>3.346900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>3.299500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>3.268100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.243200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>3.103900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>3.129500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>3.187700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>3.156700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>3.052100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>3.188600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>3.149800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>3.131800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.148800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>3.121100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>3.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>3.095800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>3.088200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.108900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>3.070100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>3.105400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>3.115800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>3.102000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.131900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>3.085700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>3.109400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>3.065500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>3.118100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.080100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7500, training_loss=3.4194492065429687, metrics={'train_runtime': 580.1089, 'train_samples_per_second': 51.714, 'train_steps_per_second': 12.929, 'total_flos': 382758912000000.0, 'train_loss': 3.4194492065429687, 'epoch': 3.0})"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "\n",
    "# Variable to control the size of the dataset used\n",
    "dataset_size = 10000 # Adjust this value to use a smaller or larger dataset\n",
    "\n",
    "# Load DistilGPT-2 and tokenizer\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad token to eos token to avoid padding issues\n",
    "\n",
    "# Define the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Subset the dataset to the desired size\n",
    "train_pairs_subset = train_pairs[:dataset_size]\n",
    "\n",
    "# Prepare dataset by encoding conversation pairs\n",
    "class ChatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, pairs, tokenizer, max_length=50):\n",
    "        self.inputs = []\n",
    "        for input_text, response_text in pairs:\n",
    "            # Encode the conversation pair with padding and truncation\n",
    "            encodings = tokenizer(\n",
    "                input_text + tokenizer.eos_token + response_text + tokenizer.eos_token,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding='max_length',  # Ensures each sequence has max_length\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            input_ids = encodings[\"input_ids\"].squeeze()\n",
    "            # Set labels to be the same as input_ids\n",
    "            self.inputs.append({\n",
    "                \"input_ids\": input_ids,\n",
    "                \"labels\": input_ids.clone()\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx]\n",
    "\n",
    "# Create the dataset\n",
    "train_dataset = ChatDataset(train_pairs_subset, tokenizer)\n",
    "\n",
    "# Define a data collator for dynamic padding\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False  # Set mlm to False for causal language modeling\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"no\",  # Set to \"steps\" or \"epoch\" if validation data is available\n",
    "    weight_decay=0.01,\n",
    "    fp16=torch.cuda.is_available(),  # Enable mixed precision if CUDA is available\n",
    ")\n",
    "\n",
    "# Use Trainer API for easy training\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator  # Use the data collator for proper padding\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline, GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load DistilGPT-2 and tokenizer\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Initialize pipelines for different NLP tasks\n",
    "text_generation = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "text_completion = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)  # Reuse for text completion\n",
    "\n",
    "# Task 1: Text Generation\n",
    "print(\"=== Text Generation ===\")\n",
    "prompt_text = \"In a world where artificial intelligence\"\n",
    "generated_texts = text_generation(prompt_text, max_length=50, num_return_sequences=1)\n",
    "for text in generated_texts:\n",
    "    print(text[\"generated_text\"])\n",
    "\n",
    "# Task 2: Text Summarization (approximation, since GPT-2 isn’t fine-tuned for summarization)\n",
    "print(\"\\n=== Text Summarization (Approximation) ===\")\n",
    "summary_prompt = (\n",
    "    \"Machine learning models are trained using data and can improve automatically through experience. \"\n",
    "    \"Recent advancements have led to significant breakthroughs in NLP, allowing models to understand and generate human language.\"\n",
    ")\n",
    "summarization_result = text_generation(summary_prompt, max_length=50, num_return_sequences=1)\n",
    "for text in summarization_result:\n",
    "    print(text[\"generated_text\"])\n",
    "\n",
    "# Task 3: Text Completion\n",
    "print(\"\\n=== Text Completion ===\")\n",
    "completion_prompt = \"The future of AI and machine learning will be shaped by\"\n",
    "completion_result = text_completion(completion_prompt, max_length=50, num_return_sequences=1)\n",
    "for text in completion_result:\n",
    "    print(text[\"generated_text\"])\n"
   ],
   "metadata": {
    "id": "GlKVN9KVVvJG",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9dc82b49-cf51-4968-ff24-d7555b1b27c2"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== Text Generation ===\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "In a world where artificial intelligence can help to change everything, it could become a reality -- and a way to get there.\n",
      "\n",
      "=== Text Summarization (Approximation) ===\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Machine learning models are trained using data and can improve automatically through experience. Recent advancements have led to significant breakthroughs in NLP, allowing models to understand and generate human language.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "It\n",
      "\n",
      "=== Text Completion ===\n",
      "The future of AI and machine learning will be shaped by an interplay between AI and machines. The result could include future efforts by AI scientists, computer scientists and engineers in AI to better understand each other, to strengthen communication between both men and women,\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load DistilGPT-2 and tokenizer for the food-themed tests\n",
    "text_generation = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Task 1: Food-Themed Text Generation\n",
    "print(\"=== Food-Themed Text Generation ===\")\n",
    "prompt_food = \"The chef prepared a delicious meal that included\"\n",
    "generated_food_texts = text_generation(prompt_food, max_length=50, num_return_sequences=1)\n",
    "for text in generated_food_texts:\n",
    "    print(text[\"generated_text\"])\n",
    "\n",
    "# Task 2: Recipe Suggestion (approximated by the text generation task)\n",
    "print(\"\\n=== Recipe Suggestion ===\")\n",
    "recipe_prompt = \"For a quick pasta dish, start with olive oil, garlic, and add\"\n",
    "recipe_suggestion = text_generation(recipe_prompt, max_length=60, num_return_sequences=1)\n",
    "for text in recipe_suggestion:\n",
    "    print(text[\"generated_text\"])\n",
    "\n",
    "# Task 3: Food Description\n",
    "print(\"\\n=== Food Description ===\")\n",
    "description_prompt = \"Describe the flavor of a freshly baked chocolate cake.\"\n",
    "food_description = text_generation(description_prompt, max_length=50, num_return_sequences=1)\n",
    "for text in food_description:\n",
    "    print(text[\"generated_text\"])\n",
    "\n",
    "# Task 4: Food Q&A\n",
    "print(\"\\n=== Food Q&A ===\")\n",
    "qa_prompt_food = \"Question: What are the best spices for a curry? Answer:\"\n",
    "qa_result_food = text_generation(qa_prompt_food, max_length=50, num_return_sequences=1)\n",
    "for text in qa_result_food:\n",
    "    print(text[\"generated_text\"])\n"
   ],
   "metadata": {
    "id": "BKoBDXMrcovg",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b2d312e2-4c43-4833-d103-724fdc1bcd9f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== Food-Themed Text Generation ===\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The chef prepared a delicious meal that included mashed potatoes and a salad with salt. It was great as an attempt at a special way to make it a dessert.\n",
      "\n",
      "\n",
      "Ingredients\n",
      "1 bag of mushrooms\n",
      "1 tbsp butter\n",
      "1 large onion,\n",
      "\n",
      "=== Recipe Suggestion ===\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For a quick pasta dish, start with olive oil, garlic, and add more oregano and onion to get a pretty good start. Just use the olive oil and pepper and mix with the onion in the pan and add another 1/2 of the oregano. Pour the pasta into a\n",
      "\n",
      "=== Food Description ===\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Describe the flavor of a freshly baked chocolate cake.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In the next paragraph we'll discuss an experiment in using peanut butter for making the chocolate cake.\n",
      "First let me know what you think about this tutorial:\n",
      "\n",
      "\n",
      "=== Food Q&A ===\n",
      "Question: What are the best spices for a curry? Answer: I enjoy it because I love it because i like it because it reminds me of curry is a great spice for a lot of the people in my house that I know. I like to\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "def fine_tune_model(model_name, train_pairs_subset, max_length=50, batch_size=4, num_epochs=3):\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token if hasattr(tokenizer, 'eos_token') else tokenizer.pad_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)  # Adjust for the correct model class\n",
    "    model.to(device)\n",
    "\n",
    "    # Prepare the dataset\n",
    "    train_dataset = ChatDataset(train_pairs_subset, tokenizer, max_length=max_length)\n",
    "\n",
    "    # Define a data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results/{model_name}',\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        logging_steps=100,\n",
    "        evaluation_strategy=\"no\",\n",
    "        weight_decay=0.01,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    # Use Trainer API for easy training\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    try:\n",
    "        trainer.train()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "\n",
    "    # Ensure all tensors are contiguous after training\n",
    "    for param in model.parameters():\n",
    "        param.data = param.data.contiguous()\n"
   ],
   "metadata": {
    "id": "7uj0TG0GddEg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pip install --upgrade transformers torch\n"
   ],
   "metadata": {
    "id": "B5t2lufpme9Z",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "2703985a-25f3-4672-a46b-5a4a4a010ddc"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
      "Collecting torch\n",
      "  Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.1.0 (from torch)\n",
      "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tokenizers, nvidia-cusolver-cu12, transformers, torch\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.6.77\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
      "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.3.3\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.3.3:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.3.3\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n",
      "    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.44.2\n",
      "    Uninstalling transformers-4.44.2:\n",
      "      Successfully uninstalled transformers-4.44.2\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.0+cu121\n",
      "    Uninstalling torch-2.5.0+cu121:\n",
      "      Successfully uninstalled torch-2.5.0+cu121\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.5.0+cu121 requires torch==2.5.0, but you have torch 2.5.1 which is incompatible.\n",
      "torchvision 0.20.0+cu121 requires torch==2.5.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 tokenizers-0.20.3 torch-2.5.1 transformers-4.46.2 triton-3.1.0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "torch",
         "torchgen",
         "transformers"
        ]
       },
       "id": "b6f3abbe6acf47cfa3d54146dd2af5a6"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Define your dataset size and prepare the pairs\n",
    "dataset_size =  10000 # Adjust as needed\n",
    "train_pairs_subset = train_pairs[:dataset_size]\n",
    "\n",
    "# Fine-tune XLNet\n",
    "fine_tune_model(\"xlnet-base-cased\", train_pairs_subset)\n",
    "\n",
    "# Fine-tune ALBERT\n",
    "fine_tune_model(\"roberta-base\", train_pairs_subset)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "4RilxUY3eB5i",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "outputId": "27b53c4b-d445-476a-8306-73abbd87b190"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'train_pairs' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-47fd63bbd13d>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Define your dataset size and prepare the pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataset_size\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;36m10000\u001b[0m \u001b[0;31m# Adjust as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_pairs_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdataset_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Fine-tune XLNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_pairs' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments, T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Ensure you have a device set up (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the dataset class\n",
    "class T5Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.inputs['input_ids'][idx],\n",
    "            'attention_mask': self.inputs['attention_mask'][idx],\n",
    "            'labels': self.labels[idx],\n",
    "        }\n",
    "\n",
    "def fine_tune_flan_t5(train_pairs_subset, model_name=\"google/flan-t5-small\", max_length=50, batch_size=4, num_epochs=3):\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "\n",
    "    # Prepare the dataset\n",
    "    inputs = tokenizer([f\"Translate English to French: {pair[0]}\" for pair in train_pairs_subset],\n",
    "                       padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    labels = tokenizer([pair[1] for pair in train_pairs_subset],\n",
    "                       padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "    # Create the dataset\n",
    "    dataset = T5Dataset(inputs, labels)\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results/{model_name}',\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        logging_steps=100,\n",
    "        evaluation_strategy=\"no\",\n",
    "        weight_decay=0.01,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    # Use Trainer API for easy training\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    try:\n",
    "        trainer.train()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "\n",
    "# Example usage with dummy data\n",
    "train_pairs_subset = [\n",
    "    (\"Hello, how are you?\", \"I'm fine, thank you!\"),\n",
    "    (\"What is your name?\", \"My name is ChatGPT.\"),\n",
    "    # Add more conversation pairs as needed\n",
    "]\n",
    "\n",
    "fine_tune_flan_t5(train_pairs_subset)\n"
   ],
   "metadata": {
    "id": "97DA5lKTsPf_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install --upgrade datasets\n"
   ],
   "metadata": {
    "id": "8P8_eXCPyA6K"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install evaluate\n"
   ],
   "metadata": {
    "id": "UMaH2Ave3GqB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(train_losses, perplexities, bleu_scores):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Perplexity\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(epochs, perplexities, label=\"Perplexity\", color=\"orange\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Perplexity\")\n",
    "    plt.title(\"Perplexity over Time\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot BLEU Score\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(epochs, bleu_scores, label=\"BLEU Score\", color=\"green\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"BLEU Score\")\n",
    "    plt.title(\"BLEU Score over Time\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage with dummy data\n",
    "train_losses = [2.3, 1.9, 1.5]  # Replace with actual loss values per epoch\n",
    "perplexities = [20, 15, 12]     # Replace with actual perplexity values per epoch\n",
    "bleu_scores = [0.25, 0.30, 0.35]  # Replace with actual BLEU scores per epoch\n",
    "\n",
    "plot_metrics(train_losses, perplexities, bleu_scores)\n"
   ],
   "metadata": {
    "id": "cfHfE6-Kidpr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define model paths\n",
    "model_paths = {\n",
    "    \"xlnet\": \"xlnet-base-cased\",\n",
    "    \"roberta\": \"roberta-base\",\n",
    "    \"flan-t5\": \"google/flan-t5-small\"\n",
    "}\n",
    "\n",
    "# Load models and tokenizers\n",
    "models = {}\n",
    "tokenizers = {}\n",
    "for name, path in model_paths.items():\n",
    "    if name == \"flan-t5\":\n",
    "        models[name] = T5ForConditionalGeneration.from_pretrained(path).eval()\n",
    "    else:\n",
    "        models[name] = AutoModelForSequenceClassification.from_pretrained(path).eval()\n",
    "    tokenizers[name] = AutoTokenizer.from_pretrained(path)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for model in models.values():\n",
    "    model.to(device)\n",
    "\n",
    "# Evaluation function for confidence and pseudo-perplexity\n",
    "def evaluate_models(models, tokenizers, sample_text):\n",
    "    results = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        tokenizer = tokenizers[name]\n",
    "        inputs = tokenizer(sample_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "        # Model-specific evaluation\n",
    "        with torch.no_grad():\n",
    "            if name != \"flan-t5\":\n",
    "                # For Sequence Classification (XLNet, RoBERTa)\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                probabilities = F.softmax(logits, dim=-1)\n",
    "                confidence = torch.max(probabilities, dim=-1).values.cpu().item()\n",
    "                results[name] = {\"confidence_score\": confidence}\n",
    "            else:\n",
    "                # For T5 (calculate pseudo-perplexity based on decoder input requirement)\n",
    "                # Shift input ids by one for decoder inputs\n",
    "                decoder_input_ids = inputs['input_ids'].clone()\n",
    "                outputs = model(input_ids=inputs['input_ids'], decoder_input_ids=decoder_input_ids)\n",
    "                logits = outputs.logits\n",
    "                loss = F.cross_entropy(\n",
    "                    logits.view(-1, logits.size(-1)),\n",
    "                    decoder_input_ids.view(-1),\n",
    "                    ignore_index=tokenizer.pad_token_id,\n",
    "                    reduction=\"mean\"\n",
    "                )\n",
    "                perplexity = torch.exp(loss).cpu().item()\n",
    "                results[name] = {\"pseudo_perplexity\": perplexity}\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test with a sample input\n",
    "sample_text = \"What are the benefits of a healthy diet?\"\n",
    "results = evaluate_models(models, tokenizers, sample_text)\n",
    "print(\"Evaluation Results:\", results)\n"
   ],
   "metadata": {
    "id": "5IzArT9O3hcz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define model paths\n",
    "model_paths = {\n",
    "    \"xlnet\": \"xlnet-base-cased\",\n",
    "    \"roberta\": \"roberta-base\",\n",
    "    \"flan-t5\": \"google/flan-t5-small\"\n",
    "}\n",
    "\n",
    "# Load models and tokenizers\n",
    "models = {}\n",
    "tokenizers = {}\n",
    "for name, path in model_paths.items():\n",
    "    if name == \"flan-t5\":\n",
    "        models[name] = T5ForConditionalGeneration.from_pretrained(path).eval()\n",
    "    else:\n",
    "        models[name] = AutoModelForSequenceClassification.from_pretrained(path).eval()\n",
    "    tokenizers[name] = AutoTokenizer.from_pretrained(path)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for model in models.values():\n",
    "    model.to(device)\n",
    "\n",
    "# Evaluation function for individual models\n",
    "def evaluate_model(model, tokenizer, text, model_name):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if model_name != \"flan-t5\":\n",
    "            # For Sequence Classification (XLNet, RoBERTa)\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            confidence = torch.max(probabilities, dim=-1).values.cpu().item()\n",
    "            return confidence\n",
    "        else:\n",
    "            # For T5 (calculate pseudo-perplexity based on decoder input requirement)\n",
    "            decoder_input_ids = inputs['input_ids'].clone()\n",
    "            outputs = model(input_ids=inputs['input_ids'], decoder_input_ids=decoder_input_ids)\n",
    "            logits = outputs.logits\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                decoder_input_ids.view(-1),\n",
    "                ignore_index=tokenizer.pad_token_id,\n",
    "                reduction=\"mean\"\n",
    "            )\n",
    "            perplexity = torch.exp(loss).cpu().item()\n",
    "            return perplexity\n",
    "\n",
    "# Ensemble evaluation function\n",
    "def ensemble_evaluate(models, tokenizers, text):\n",
    "    results = {}\n",
    "    total_confidence = 0\n",
    "    total_perplexity = 0\n",
    "    count = 0\n",
    "\n",
    "    for name, model in models.items():\n",
    "        score = evaluate_model(model, tokenizers[name], text, name)\n",
    "\n",
    "        if name != \"flan-t5\":\n",
    "            results[name] = {\"confidence_score\": score}\n",
    "            total_confidence += score\n",
    "        else:\n",
    "            results[name] = {\"pseudo_perplexity\": score}\n",
    "            total_perplexity += score\n",
    "        count += 1\n",
    "\n",
    "    # Calculate the average metrics for ensemble performance\n",
    "    avg_confidence = total_confidence / (count - 1)  # -1 as T5 gives perplexity\n",
    "    avg_perplexity = total_perplexity / 1  # Only one perplexity score (from T5)\n",
    "\n",
    "    # Final combined results\n",
    "    results[\"ensemble\"] = {\n",
    "        \"average_confidence_score\": avg_confidence,\n",
    "        \"average_pseudo_perplexity\": avg_perplexity\n",
    "    }\n",
    "    return results\n",
    "\n",
    "# Sample input\n",
    "sample_text = \"What are the benefits of a healthy diet?\"\n",
    "results = ensemble_evaluate(models, tokenizers, sample_text)\n",
    "print(\"Ensemble Evaluation Results:\", results)\n"
   ],
   "metadata": {
    "id": "50yDFWTt3mnk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n"
   ],
   "metadata": {
    "id": "2VrrGLbkjguF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define paths to the data files\n",
    "lines_file_path = 'movie_lines.txt'\n",
    "conversations_file_path = 'movie_conversations.txt'\n",
    "\n",
    "# Dynamic dataset size parameter\n",
    "dataset_size = 50  # Adjust this value for testing\n"
   ],
   "metadata": {
    "id": "-k3lqS50kZOE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Function to load and process movie lines\n",
    "def load_movie_lines(file_path):\n",
    "    id_to_line = {}\n",
    "    with open(file_path, 'r', encoding='iso-8859-1') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(\" +++$+++ \")\n",
    "            if len(parts) == 5:\n",
    "                line_id, text = parts[0], parts[4]\n",
    "                # Clean text\n",
    "                text = re.sub(r\"([?.!,])\", r\" \\1 \", text)  # Space out punctuation\n",
    "                text = re.sub(r'\\s+', ' ', text).strip()\n",
    "                id_to_line[line_id] = text\n",
    "    return id_to_line\n",
    "\n",
    "# Function to load and process conversations with a limit on the number of samples\n",
    "def load_conversations(file_path, id_to_line, limit=None):\n",
    "    conversations = []\n",
    "    with open(file_path, 'r', encoding='iso-8859-1') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(\" +++$+++ \")\n",
    "            if len(parts) == 4:\n",
    "                line_ids = eval(parts[3])  # Converts string list to list\n",
    "                conv = [id_to_line[line_id] for line_id in line_ids if line_id in id_to_line]\n",
    "                if len(conv) > 1:\n",
    "                    conversations.append(conv)\n",
    "                if limit and len(conversations) >= limit:\n",
    "                    break\n",
    "    return conversations\n"
   ],
   "metadata": {
    "id": "uw93A2wSsIHA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load movie lines and conversations with a dataset size limit\n",
    "id_to_line = load_movie_lines(lines_file_path)\n",
    "conversations = load_conversations(conversations_file_path, id_to_line, limit=dataset_size)\n",
    "\n",
    "# Create training pairs: (input, response) for each conversation\n",
    "train_pairs = []\n",
    "for conv in conversations:\n",
    "    for i in range(len(conv) - 1):\n",
    "        input_text, response_text = conv[i], conv[i + 1]\n",
    "        train_pairs.append((input_text, response_text))\n",
    "\n",
    "print(\"Total training pairs:\", len(train_pairs))\n"
   ],
   "metadata": {
    "id": "D-xr37dYsU8p"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_names = {\n",
    "    \"xlnet\": \"xlnet-base-cased\",\n",
    "    \"roberta\": \"roberta-base\",\n",
    "    \"flant5\": \"google/flan-t5-small\"\n",
    "}\n",
    "\n",
    "tokenizers = {}\n",
    "models = {}\n",
    "\n",
    "for name, model_path in model_names.items():\n",
    "    tokenizers[name] = AutoTokenizer.from_pretrained(model_path)\n",
    "    # Use AutoModelForSeq2SeqLM for \"flant5\"\n",
    "    if name == \"flant5\":\n",
    "        from transformers import AutoModelForSeq2SeqLM # Import the correct model class\n",
    "        models[name] = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "    else:\n",
    "        models[name] = AutoModelForCausalLM.from_pretrained(model_path)"
   ],
   "metadata": {
    "id": "c95LsNltsoU0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Prepare dataset for training\n",
    "class ChatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, pairs, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pairs = pairs\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text, response_text = self.pairs[idx]\n",
    "        inputs = self.tokenizer(input_text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        labels = self.tokenizer(response_text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "        labels = labels['input_ids'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n"
   ],
   "metadata": {
    "id": "rVLYxqSZtGCO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Training function for each model\n",
    "def train_model(model, tokenizer, train_data):\n",
    "    dataset = ChatDataset(train_data, tokenizer)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',          # Output directory for model predictions and checkpoints\n",
    "        per_device_train_batch_size=4,   # Batch size for training\n",
    "        num_train_epochs=3,              # Total number of training epochs\n",
    "        logging_dir='./logs',            # Directory for storing logs\n",
    "        logging_steps=10,                 # Log every 10 steps\n",
    "        save_steps=500,                   # Save model every 500 steps\n",
    "        save_total_limit=2,               # Limit the total amount of checkpoints\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n"
   ],
   "metadata": {
    "id": "jo0oKQKLtLJH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Train each model\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name} model...\")\n",
    "    train_model(model, tokenizers[name], train_pairs)\n",
    "\n",
    "print(\"Training completed!\")\n"
   ],
   "metadata": {
    "id": "jMRSGaXKuJU9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_response(model, tokenizer, input_text, max_length=50):\n",
    "    # Encode the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "    # Generate output from the model\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
    "\n",
    "    # Decode the generated output\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return response\n"
   ],
   "metadata": {
    "id": "Kv6o6U8b0beN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_models(models, tokenizers, prompts):\n",
    "    results = {}\n",
    "    for name in models.keys():\n",
    "        responses = []\n",
    "        for prompt in prompts:\n",
    "            response = generate_response(models[name], tokenizers[name], prompt)\n",
    "            responses.append(response)\n",
    "        results[name] = responses\n",
    "    return results\n"
   ],
   "metadata": {
    "id": "lHbeS8hm0gZS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "prompts = [\n",
    "    \"What are the benefits of exercise?\",\n",
    "    \"Tell me a story about a brave knight.\",\n",
    "    \"What is the future of AI?\",\n",
    "    \"Explain the importance of a healthy diet.\"\n",
    "]\n",
    "\n",
    "# Evaluate models on the given prompts\n",
    "evaluation_results = evaluate_models(models, tokenizers, prompts)\n",
    "\n",
    "# Print the results\n",
    "for model_name, responses in evaluation_results.items():\n",
    "    print(f\"Responses from {model_name}:\")\n",
    "    for prompt, response in zip(prompts, responses):\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Response: {response}\\n\")\n"
   ],
   "metadata": {
    "id": "k1MwMZN0ppA0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "8hNeAw3O0lb4"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}